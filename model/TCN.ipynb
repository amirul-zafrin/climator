{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXfOmYb6Vib8",
        "outputId": "df54a1ee-3676-4f87-cb34-480f09510fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 26 00:48:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from typing import List\n",
        "\n",
        "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n",
        "from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization\n",
        "\n",
        "\n",
        "def is_power_of_two(num: int):\n",
        "    return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "\n",
        "def adjust_dilations(dilations: list):\n",
        "    if all([is_power_of_two(i) for i in dilations]):\n",
        "        return dilations\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        return new_dilations\n",
        "\n",
        "\n",
        "class ResidualBlock(Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dilation_rate: int,\n",
        "                 nb_filters: int,\n",
        "                 kernel_size: int,\n",
        "                 padding: str,\n",
        "                 activation: str = 'relu',\n",
        "                 dropout_rate: float = 0,\n",
        "                 kernel_initializer: str = 'he_normal',\n",
        "                 use_batch_norm: bool = False,\n",
        "                 use_layer_norm: bool = False,\n",
        "                 use_weight_norm: bool = False,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.nb_filters = nb_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.use_weight_norm = use_weight_norm\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.layers = []\n",
        "        self.shape_match_conv = None\n",
        "        self.res_output_shape = None\n",
        "        self.final_activation = None\n",
        "\n",
        "        super(ResidualBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def _build_layer(self, layer):\n",
        "       \n",
        "        self.layers.append(layer)\n",
        "        self.layers[-1].build(self.res_output_shape)\n",
        "        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        with K.name_scope(self.name):  \n",
        "            self.layers = []\n",
        "            self.res_output_shape = input_shape\n",
        "\n",
        "            for k in range(2):\n",
        "                name = 'conv1D_{}'.format(k)\n",
        "                with K.name_scope(name):  \n",
        "                    conv = Conv1D(\n",
        "                        filters=self.nb_filters,\n",
        "                        kernel_size=self.kernel_size,\n",
        "                        dilation_rate=self.dilation_rate,\n",
        "                        padding=self.padding,\n",
        "                        name=name,\n",
        "                        kernel_initializer=self.kernel_initializer\n",
        "                    )\n",
        "                    if self.use_weight_norm:\n",
        "                        from tensorflow_addons.layers import WeightNormalization\n",
        "                        with K.name_scope('norm_{}'.format(k)):\n",
        "                            conv = WeightNormalization(conv)\n",
        "                    self._build_layer(conv)\n",
        "\n",
        "                with K.name_scope('norm_{}'.format(k)):\n",
        "                    if self.use_batch_norm:\n",
        "                        self._build_layer(BatchNormalization())\n",
        "                    elif self.use_layer_norm:\n",
        "                        self._build_layer(LayerNormalization())\n",
        "                    elif self.use_weight_norm:\n",
        "                        pass \n",
        "\n",
        "                with K.name_scope('act_and_dropout_{}'.format(k)):\n",
        "                    self._build_layer(Activation(self.activation, name='Act_Conv1D_{}'.format(k)))\n",
        "                    self._build_layer(SpatialDropout1D(rate=self.dropout_rate, name='SDropout_{}'.format(k)))\n",
        "\n",
        "            if self.nb_filters != input_shape[-1]:\n",
        "                name = 'matching_conv1D'\n",
        "                with K.name_scope(name):\n",
        "                    self.shape_match_conv = Conv1D(\n",
        "                        filters=self.nb_filters,\n",
        "                        kernel_size=1,\n",
        "                        padding='same',\n",
        "                        name=name,\n",
        "                        kernel_initializer=self.kernel_initializer\n",
        "                    )\n",
        "            else:\n",
        "                name = 'matching_identity'\n",
        "                self.shape_match_conv = Lambda(lambda x: x, name=name)\n",
        "\n",
        "            with K.name_scope(name):\n",
        "                self.shape_match_conv.build(input_shape)\n",
        "                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n",
        "\n",
        "            self._build_layer(Activation(self.activation, name='Act_Conv_Blocks'))\n",
        "            self.final_activation = Activation(self.activation, name='Act_Res_Block')\n",
        "            self.final_activation.build(self.res_output_shape)  \n",
        "\n",
        "  \n",
        "            for layer in self.layers:\n",
        "                self.__setattr__(layer.name, layer)\n",
        "            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n",
        "            self.__setattr__(self.final_activation.name, self.final_activation)\n",
        "\n",
        "            super(ResidualBlock, self).build(input_shape) \n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        x1 = inputs\n",
        "        for layer in self.layers:\n",
        "            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n",
        "            x1 = layer(x1, training=training) if training_flag else layer(x1)\n",
        "        x2 = self.shape_match_conv(inputs)\n",
        "        x1_x2 = self.final_activation(layers.add([x2, x1], name='Add_Res'))\n",
        "        return [x1_x2, x1]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [self.res_output_shape, self.res_output_shape]\n",
        "\n",
        "\n",
        "class TCN(Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=3,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=(1, 2, 4, 8, 16, 32),\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=False,\n",
        "                 activation='relu',\n",
        "                 kernel_initializer='he_normal',\n",
        "                 use_batch_norm=False,\n",
        "                 use_layer_norm=False,\n",
        "                 use_weight_norm=False,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.activation = activation\n",
        "        self.padding = padding\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.use_weight_norm = use_weight_norm\n",
        "        self.skip_connections = []\n",
        "        self.residual_blocks = []\n",
        "        self.layers_outputs = []\n",
        "        self.build_output_shape = None\n",
        "        self.slicer_layer = None  \n",
        "        self.output_slice_index = None \n",
        "        self.padding_same_and_time_dim_unknown = False \n",
        "\n",
        "        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n",
        "            raise ValueError('Only one normalization can be specified at once.')\n",
        "\n",
        "        if isinstance(self.nb_filters, list):\n",
        "            assert len(self.nb_filters) == len(self.dilations)\n",
        "            if len(set(self.nb_filters)) > 1 and self.use_skip_connections:\n",
        "                raise ValueError('Skip connections are not compatible '\n",
        "                                 'with a list of filters, unless they are all equal.')\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "        super(TCN, self).__init__(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def receptive_field(self):\n",
        "        return 1 + 2 * (self.kernel_size - 1) * self.nb_stacks * sum(self.dilations)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.build_output_shape = input_shape\n",
        "\n",
        "        self.residual_blocks = []\n",
        "        total_num_blocks = self.nb_stacks * len(self.dilations)\n",
        "        if not self.use_skip_connections:\n",
        "            total_num_blocks += 1  \n",
        "\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i, d in enumerate(self.dilations):\n",
        "                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n",
        "                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n",
        "                                                          nb_filters=res_block_filters,\n",
        "                                                          kernel_size=self.kernel_size,\n",
        "                                                          padding=self.padding,\n",
        "                                                          activation=self.activation,\n",
        "                                                          dropout_rate=self.dropout_rate,\n",
        "                                                          use_batch_norm=self.use_batch_norm,\n",
        "                                                          use_layer_norm=self.use_layer_norm,\n",
        "                                                          use_weight_norm=self.use_weight_norm,\n",
        "                                                          kernel_initializer=self.kernel_initializer,\n",
        "                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n",
        "                \n",
        "                self.residual_blocks[-1].build(self.build_output_shape)\n",
        "                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n",
        "\n",
        "        for layer in self.residual_blocks:\n",
        "            self.__setattr__(layer.name, layer)\n",
        "\n",
        "        self.output_slice_index = None\n",
        "        if self.padding == 'same':\n",
        "            time = self.build_output_shape.as_list()[1]\n",
        "            if time is not None:\n",
        "                self.output_slice_index = int(self.build_output_shape.as_list()[1] / 2)\n",
        "            else:\n",
        "                self.padding_same_and_time_dim_unknown = True\n",
        "\n",
        "        else:\n",
        "            self.output_slice_index = -1 \n",
        "        self.slicer_layer = Lambda(lambda tt: tt[:, self.output_slice_index, :], name='Slice_Output')\n",
        "        self.slicer_layer.build(self.build_output_shape.as_list())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if not self.built:\n",
        "            self.build(input_shape)\n",
        "        if not self.return_sequences:\n",
        "            batch_size = self.build_output_shape[0]\n",
        "            batch_size = batch_size.value if hasattr(batch_size, 'value') else batch_size\n",
        "            nb_filters = self.build_output_shape[-1]\n",
        "            return [batch_size, nb_filters]\n",
        "        else:\n",
        "            return [v.value if hasattr(v, 'value') else v for v in self.build_output_shape]\n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "        x = inputs\n",
        "        self.layers_outputs = [x]\n",
        "        self.skip_connections = []\n",
        "        for res_block in self.residual_blocks:\n",
        "            try:\n",
        "                x, skip_out = res_block(x, training=training)\n",
        "            except TypeError:\n",
        "                x, skip_out = res_block(K.cast(x, 'float32'), training=training)\n",
        "            self.skip_connections.append(skip_out)\n",
        "            self.layers_outputs.append(x)\n",
        "\n",
        "        if self.use_skip_connections:\n",
        "            x = layers.add(self.skip_connections, name='Add_Skip_Connections')\n",
        "            self.layers_outputs.append(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            if self.padding_same_and_time_dim_unknown:\n",
        "                self.output_slice_index = K.shape(self.layers_outputs[-1])[1] // 2\n",
        "            x = self.slicer_layer(x)\n",
        "            self.layers_outputs.append(x)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TCN, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        config['kernel_size'] = self.kernel_size\n",
        "        config['nb_stacks'] = self.nb_stacks\n",
        "        config['dilations'] = self.dilations\n",
        "        config['padding'] = self.padding\n",
        "        config['use_skip_connections'] = self.use_skip_connections\n",
        "        config['dropout_rate'] = self.dropout_rate\n",
        "        config['return_sequences'] = self.return_sequences\n",
        "        config['activation'] = self.activation\n",
        "        config['use_batch_norm'] = self.use_batch_norm\n",
        "        config['use_layer_norm'] = self.use_layer_norm\n",
        "        config['use_weight_norm'] = self.use_weight_norm\n",
        "        config['kernel_initializer'] = self.kernel_initializer\n",
        "        return config\n",
        "\n",
        "\n",
        "def compiled_tcn(num_feat, \n",
        "                 num_classes,  \n",
        "                 nb_filters, \n",
        "                 kernel_size, \n",
        "                 dilations,  \n",
        "                 nb_stacks,\n",
        "                 max_len,\n",
        "                 output_len=1,  \n",
        "                 padding='causal',  \n",
        "                 use_skip_connections=False, \n",
        "                 return_sequences=True,\n",
        "                 regression=False,  \n",
        "                 dropout_rate=0.5, \n",
        "                 name='tcn',\n",
        "                 kernel_initializer='he_normal', \n",
        "                 activation='relu',\n",
        "                 opt='adam',\n",
        "                 lr=0.002,\n",
        "                 use_batch_norm=False,\n",
        "                 use_layer_norm=False,\n",
        "                 use_weight_norm=False):\n",
        "    \n",
        "    dilations = adjust_dilations(dilations)\n",
        "\n",
        "    input_layer = Input(shape=(max_len, num_feat))\n",
        "\n",
        "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n",
        "            use_skip_connections, dropout_rate, return_sequences,\n",
        "            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n",
        "            use_weight_norm, name=name)(input_layer)\n",
        "\n",
        "    print('x.shape=', x.shape)\n",
        "\n",
        "    def get_opt():\n",
        "        if opt == 'adam':\n",
        "            return optimizers.Adam(lr=lr, clipnorm=1.)\n",
        "        elif opt == 'rmsprop':\n",
        "            return optimizers.RMSprop(lr=lr, clipnorm=1.)\n",
        "        else:\n",
        "            raise Exception('Only Adam and RMSProp are available here')\n",
        "\n",
        "    if not regression:\n",
        "        x = Dense(num_classes)(x)\n",
        "        x = Activation('softmax')(x)\n",
        "        output_layer = x\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        def accuracy(y_true, y_pred):\n",
        "            if K.ndim(y_true) == K.ndim(y_pred):\n",
        "                y_true = K.squeeze(y_true, -1)\n",
        "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
        "    else:\n",
        "        # regression\n",
        "        x = Dense(output_len)(x)\n",
        "        x = Activation('linear')(x)\n",
        "        output_layer = x\n",
        "        model = Model(input_layer, output_layer)\n",
        "        model.compile(get_opt(), loss='mean_squared_error')\n",
        "    print('model.x = {}'.format(input_layer.shape))\n",
        "    print('model.y = {}'.format(output_layer.shape))\n",
        "    return model\n",
        "\n",
        "\n",
        "def tcn_full_summary(model: Model, expand_residual_blocks=True):\n",
        "    import tensorflow as tf\n",
        "    # 2.6.0-rc1, 2.5.0...\n",
        "    versions = [int(v) for v in tf.__version__.split('-')[0].split('.')]\n",
        "    if versions[0] <= 2 and versions[1] < 5:\n",
        "        layers = model._layers.copy()  # store existing layers\n",
        "        model._layers.clear()  # clear layers\n",
        "\n",
        "        for i in range(len(layers)):\n",
        "            if isinstance(layers[i], TCN):\n",
        "                for layer in layers[i]._layers:\n",
        "                    if not isinstance(layer, ResidualBlock):\n",
        "                        if not hasattr(layer, '__iter__'):\n",
        "                            model._layers.append(layer)\n",
        "                    else:\n",
        "                        if expand_residual_blocks:\n",
        "                            for lyr in layer._layers:\n",
        "                                if not hasattr(lyr, '__iter__'):\n",
        "                                    model._layers.append(lyr)\n",
        "                        else:\n",
        "                            model._layers.append(layer)\n",
        "            else:\n",
        "                model._layers.append(layers[i])\n",
        "\n",
        "        model.summary()  # print summary\n",
        "\n",
        "        # restore original layers\n",
        "        model._layers.clear()\n",
        "        [model._layers.append(lyr) for lyr in layers]\n",
        "    else:\n",
        "      pass"
      ],
      "metadata": {
        "id": "9HcPVOxJVkb0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, time_steps, input_dim = 128, 20, 1"
      ],
      "metadata": {
        "id": "VRz4R4aIVwpl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('temperature_filtered.csv', header=None)\n",
        "\n",
        "data = data[6]\n",
        "train_data = data[:int(len(data)*0.8)]\n",
        "test_data = data[int(len(data)*0.8):]\n",
        "\n",
        "def df_to_XY(df, window_size = 5):\n",
        "    df_as_np = df.to_numpy()\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(df_as_np) - window_size):\n",
        "        row = [[a] for a in df_as_np[i:i+window_size]]\n",
        "        x.append(row)\n",
        "        label = df_as_np[i+window_size]\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "\n",
        "X, y = df_to_XY(data, window_size = time_steps)\n",
        "\n",
        "split_ratio = round(len(X)*0.8)\n",
        "\n",
        "X_train, y_train = X[:split_ratio], y[:split_ratio]\n",
        "X_test, y_test = X[split_ratio:], y[split_ratio:]"
      ],
      "metadata": {
        "id": "tf8cFA4VV46T"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_layer = TCN(input_shape=(time_steps, input_dim))\n",
        "# The receptive field tells you how far the model can see in terms of timesteps.\n",
        "print('Receptive field size =', tcn_layer.receptive_field)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75EaKtThWTu4",
        "outputId": "ca147c9f-adda-4e20-d82b-f29dca81a0c8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receptive field size = 253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "H2BPgY5yWrnM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "m = keras.Sequential([\n",
        "    tcn_layer,\n",
        "    Dense(32),\n",
        "    Dense(12),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "m.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "IiF9RXRTWcLQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.fit(X_train, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYAWvbIPWxPK",
        "outputId": "53b63830-d6e4-4b92-da91-46a9cb696097"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6703/6703 [==============================] - 59s 9ms/step - loss: 5.7283 - val_loss: 5.0085\n",
            "Epoch 2/10\n",
            "6703/6703 [==============================] - 56s 8ms/step - loss: 5.5339 - val_loss: 5.3262\n",
            "Epoch 3/10\n",
            "6703/6703 [==============================] - 56s 8ms/step - loss: 5.5101 - val_loss: 5.0746\n",
            "Epoch 4/10\n",
            "6703/6703 [==============================] - 67s 10ms/step - loss: 5.4902 - val_loss: 4.9314\n",
            "Epoch 5/10\n",
            "6703/6703 [==============================] - 72s 11ms/step - loss: 5.4794 - val_loss: 5.3789\n",
            "Epoch 6/10\n",
            "6703/6703 [==============================] - 75s 11ms/step - loss: 5.4689 - val_loss: 5.0098\n",
            "Epoch 7/10\n",
            "6703/6703 [==============================] - 57s 8ms/step - loss: 5.4566 - val_loss: 4.9495\n",
            "Epoch 8/10\n",
            "6703/6703 [==============================] - 55s 8ms/step - loss: 5.4467 - val_loss: 5.0877\n",
            "Epoch 9/10\n",
            "6703/6703 [==============================] - 56s 8ms/step - loss: 5.4415 - val_loss: 5.8571\n",
            "Epoch 10/10\n",
            "6703/6703 [==============================] - 57s 9ms/step - loss: 5.4418 - val_loss: 4.9615\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6968d3ca90>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if61AUyZW8Oa",
        "outputId": "9bfeedc0-b7c0-4000-b6dc-80970335fcab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tcn_2 (TCN)                 (None, 64)                136256    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                396       \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 13        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,745\n",
            "Trainable params: 138,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.save(\"/TCN_Model_V2\", save_format = \"h5\")"
      ],
      "metadata": {
        "id": "gCYJI6sHZzEa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.evaluate(X_test, y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVeLXZ2baxtO",
        "outputId": "7896193c-e99e-45c0-a22a-eea07552c5f2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2095/2095 - 7s - loss: 3.7735 - 7s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.773507833480835"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XB_MAu9RfdY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}